import streamlit as st 
from naraetool.utils import *

setting()

# llm ê´€ë ¨
from pathlib import Path 
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from operator import itemgetter

#--------------------------------------------------------------------------
## Settings
#--------------------------------------------------------------------------
session_key = "chat_history"

def read_isfj(x):
    isfj_path = Path("./docs/isfj.txt")
    return isfj_path.read_text(encoding="utf-8")

template = """\
# INSTRUCTION
- ë‹¹ì‹ ì˜ MBTIëŠ” ISFJì…ë‹ˆë‹¤. 
- ë‹¹ì‹ ì˜ ì„±ê²©ì€ PERSONALITYì™€ ê°™ìŠµë‹ˆë‹¤.
- PERSONALITYì— ë§ì¶° USERì— ë‹µë³€í•˜ì„¸ìš”.

# PERSONALITY: {personality}

# USER: {input}
"""
#--------------------------------------------------------------------------
## Load Session Information
#--------------------------------------------------------------------------
if session_key not in st.session_state:
    st.session_state[session_key] = []

if "chain" in st.session_state:
    chain = st.session_state["chain"]
    memory = st.session_state["memory"]
else:
    # Chat ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    # prompt = PromptTemplate.from_template(template)
    memory = ConversationBufferMemory(
        return_messages=True, 
        memory_key="chat_history"
    )
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )
    runnable = RunnablePassthrough.assign(
        chat_history = RunnableLambda(memory.load_memory_variables)
        | itemgetter("chat_history")
    )
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash")
    output_parser = StrOutputParser()
    runnable1 = {"input": RunnablePassthrough()}
    runnable2 = RunnablePassthrough.assign(
            chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history"),
            personality=RunnableLambda(read_isfj)
        )
    runnable = runnable1 | runnable2
    chain = runnable | prompt | model | StrOutputParser()
    st.session_state["chain"] = chain
    st.session_state["memory"] = memory

#--------------------------------------------------------------------------
## Header & Body
#--------------------------------------------------------------------------
# ì²« ì±„íŒ…ì„ ì‹œì‘í•  ë•Œ ì²« ì¸ì‚¬ ì¶œë ¥
if len(st.session_state[session_key]) == 0:
    greeting = "ì•ˆë…•í•˜ì„¸ìš”. ì œ MBTIëŠ” ISFJì…ë‹ˆë‹¤.ğŸ˜Š"
    st.chat_message("assistant").markdown(greeting)
    st.session_state[session_key].append(
        {"role":"assistant", "content":greeting}
    )

# ì±„íŒ… ê¸°ë¡ì´ ìˆì„ ë•Œ ê¸°ë¡ëœ ì±„íŒ… ì¶œë ¥
else:
    for chat in st.session_state[session_key]:
        st.chat_message(chat["role"]).markdown(chat["content"])

question = st.chat_input(placeholder="ë©”ì„¸ì§€ ì…ë ¥")

# ì±„íŒ…ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œ
if question:
    # ì…ë ¥ëœ ì±„íŒ… ì¶œë ¥
    st.chat_message("user").markdown(question)
    st.session_state[session_key].append(
        {"role":"user", "content":question}
    )
    
    # ë‹µë³€ ì¶œë ¥
    with st.chat_message("assistant"):
        container = st.empty()
        answer = ""
        for token in chain.invoke(question):
            answer += token
            container.markdown(answer)
            
    st.session_state[session_key].append({"role":"assistant", "content":answer})
    # ë©”ëª¨ë¦¬ ì €ì¥
    memory.save_context(
        {"inputs": question},
        {"output": answer}
    )

	# ë©”ëª¨ë¦¬ ì¶œë ¥
    print(memory.load_memory_variables({}))