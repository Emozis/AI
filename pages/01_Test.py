import streamlit as st 
from naraetool.utils import *
import json

setting()

# llm ê´€ë ¨
from pathlib import Path 
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from operator import itemgetter

#--------------------------------------------------------------------------
## Settings
#--------------------------------------------------------------------------
session_key = "chat_history"

temp = """\
## INSTRUCTION
- ë‹¹ì‹ ì€ ë°°ìš°ì…ë‹ˆë‹¤. ë°°ìš°ì˜ ì •ë³´ëŠ” INFORMATIONì„ ì°¸ê³ í•˜ì„¸ìš”.
- ìºë¦­í„°ì— ëª°ì…í•´ì„œ ë§íˆ¬ì™€ ì„±ê²©ì„ ìœ ì§€í•˜ì„¸ìš”.
- ì „ì— í–ˆë˜ ëŒ€í™”ë¥¼ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
- ë„ˆë¬´ ì§§ê²Œ ëŒ€ë‹µí•˜ì§€ ë§ˆì„¸ìš”.
- INFORMATION ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ INPUTì— ë‹µí•˜ì„¸ìš”.

## INFORMATION
- NAME: {name}
- GENDER: {gender}
- RELATIONSHIP: {relationship}
- PERSONALITY: {personality}
- DETAILS: {details}
"""

temp_var = {
    "name":"ë¶€ì¥ë‹˜",
    "gender":"ì—¬",
    "relationship":"ë¶€ì¥ë‹˜",
    "personality":"ê¹Œë‹¤ë¡œì›€, ë‚ ì¹´ë¡œì›€",
    "details": "ì•„ì´ë¥¼ ì¢‹ì•„í•¨"
}

with st.expander(label=":gear: Settings", expanded=False):
    template = st.text_area(label="Template", value=temp, height=300)
    variable = st.text_area(label="Input_variable", value=temp_var, height=100)
    save_button = st.button(label="Save", use_container_width=True, type="primary")
    if save_button:
        # Chat ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        # prompt = PromptTemplate.from_template(template)
        memory = ConversationBufferMemory(
            return_messages=True, 
            memory_key="chat_history"
        )
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", template),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
            ]
        )
        runnable = RunnablePassthrough.assign(
            chat_history = RunnableLambda(memory.load_memory_variables)
            | itemgetter("chat_history")
        )
        model = ChatGoogleGenerativeAI(model="gemini-1.5-flash")
        output_parser = StrOutputParser()
        runnable = RunnablePassthrough.assign(
                chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")
            )
        chain = runnable | prompt | model | StrOutputParser()
        st.session_state["chain"] = chain
        st.session_state["memory"] = memory
#--------------------------------------------------------------------------
## Load Session Information
#--------------------------------------------------------------------------
if session_key not in st.session_state:
    st.session_state[session_key] = []

if "chain" in st.session_state:
    chain = st.session_state["chain"]
    memory = st.session_state["memory"]
else:
    # Chat ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    # prompt = PromptTemplate.from_template(template)
    memory = ConversationBufferMemory(
        return_messages=True, 
        memory_key="chat_history"
    )
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )
    runnable = RunnablePassthrough.assign(
        chat_history = RunnableLambda(memory.load_memory_variables)
        | itemgetter("chat_history")
    )
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash")
    output_parser = StrOutputParser()
    runnable = RunnablePassthrough.assign(
            chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")
        )
    chain = runnable | prompt | model | StrOutputParser()
    st.session_state["chain"] = chain
    st.session_state["memory"] = memory

#--------------------------------------------------------------------------
## Header & Body
#--------------------------------------------------------------------------
# ì²« ì±„íŒ…ì„ ì‹œì‘í•  ë•Œ ì²« ì¸ì‚¬ ì¶œë ¥
if len(st.session_state[session_key]) == 0:
    greeting = "ì•ˆë…•í•˜ì„¸ìš”.ğŸ˜Š"
    st.chat_message("assistant").markdown(greeting)
    st.session_state[session_key].append(
        {"role":"assistant", "content":greeting}
    )

# ì±„íŒ… ê¸°ë¡ì´ ìˆì„ ë•Œ ê¸°ë¡ëœ ì±„íŒ… ì¶œë ¥
else:
    for chat in st.session_state[session_key]:
        st.chat_message(chat["role"]).markdown(chat["content"])

question = st.chat_input(placeholder="ë©”ì„¸ì§€ ì…ë ¥")

# ì±„íŒ…ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œ
if question:
    # ì…ë ¥ëœ ì±„íŒ… ì¶œë ¥
    st.chat_message("user").markdown(question)
    st.session_state[session_key].append(
        {"role":"user", "content":question}
    )
    
    # ë‹µë³€ ì¶œë ¥
    with st.chat_message("assistant"):
        container = st.empty()
        answer = ""
        var_json = json.loads(variable.replace('\'','\"'))
        var_json["input"] = input
        answer = chain.invoke(var_json)
        st.markdown(answer)
        # for token in chain.stream(var_json):
        #     answer += token
        #     container.markdown(answer)
            
    st.session_state[session_key].append({"role":"assistant", "content":answer})
    # ë©”ëª¨ë¦¬ ì €ì¥
    memory.save_context(
        {"inputs": question},
        {"output": answer}
    )

	# ë©”ëª¨ë¦¬ ì¶œë ¥
    print(memory.load_memory_variables({}))