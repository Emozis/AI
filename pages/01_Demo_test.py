import streamlit as st 
from naraetool.utils import *
from naraetool.langchain import *

setting()

# llm ê´€ë ¨
from pathlib import Path 
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from operator import itemgetter

from google.api_core import exceptions
#-------------------------------------------------------------------
# Settings
#-------------------------------------------------------------------
key_expander = "is_expand"
key_chain = "demo_chain"
key_greeting = "demo_greeting"

def fold_container():
    st.session_state[key_expander] = False

# ìƒíƒœ ì´ˆê¸°í™”
if not key_expander in st.session_state:
    st.session_state[key_expander] = True

#-------------------------------------------------------------------
# Header
#-------------------------------------------------------------------

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
import asyncio
import nest_asyncio
nest_asyncio.apply()

# result = asyncio.run(main())
# print(f"RESULT: {result}")

async def get_response(input):
    template = "ì¹œêµ¬ì²˜ëŸ¼ ëŒ€ë‹µí•´ì£¼ì„¸ìš”"
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", template),
            # MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )
    model = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            temperature=0.7
        )
    # ì¶œë ¥ íŒŒì„œ ì„¤ì •
    output_parser = StrOutputParser()

    # ì²´ì¸ ë§Œë“¤ê¸°
    chain = prompt | model | output_parser

    result = chain.astream({"input":input})
    async for token in result:
        # í•œê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë°
        for char in token:
            await asyncio.sleep(0.01)
            yield char

async def main():
    container = st.empty()
    output = ""
    async for char in get_response("ë„Œ ëˆ„êµ¬ì•¼"):
        output += char
        container.markdown(output)
        print(char, end="", flush=True)
    print()  # ë§ˆì§€ë§‰ì— ì¤„ë°”ê¿ˆ

    return output

asyncio.run(main())


# with st.expander(
#         label=":gear: Settigns", 
#         expanded=st.session_state[key_expander]
#     ):  
#     # Prompt Select Box
#     more_text = "â• ì§ì ‘ ì…ë ¥"
#     path = Path("./static/persona")
#     files = {file.stem:file for file in sorted(path.iterdir())}

#     select = st.selectbox(
#         label="PROMPT",
#         options=list(files.keys()) + [more_text]
#     )
    
#     # Print Template
#     if select == more_text:
#         persona = st.text_area(
#             label="TEMPLATE",
#                 value="",
#                 height=300
#         )
#     else:
#         with open(files[select], 'r', encoding="utf-8") as txt_file:
#             persona = st.text_area(
#                 label="TEMPLATE",
#                 value=txt_file.read(),
#                 height=300,
#                 disabled=True
#             )
        
#     # Chat Start Button
#     start_btn = st.button(
#         label="CHAT START",
#         use_container_width=True,
#         type="primary",
#         on_click=fold_container,
#         args=""
#     )

# #-------------------------------------------------------------------
# # Make Chain
# #-------------------------------------------------------------------
# # ì±„íŒ…ì„ ì´ì–´ë‚˜ê°ˆ ë•Œ
# if key_chain in st.session_state:
#     chain = st.session_state[key_chain]
#     memory = st.session_state[key_memory]
#     history = st.session_state[key_history]
# # ìƒˆë¡œìš´ í…œí”Œë¦¿ì„ ì ìš©í•  ë•Œ
# else:
#     # ì´ˆê¸°í™” 
#     st.session_state[key_history] = []

#     # ë©”ëª¨ë¦¬ ì„¤ì •
#     memory = ConversationBufferMemory(
#         return_messages=True, 
#         memory_key="chat_history"
#     )
#     # í”„ë¡¬í”„íŠ¸ ì„¤ì •
#     template = read_prompt("./static/templates/Demo.prompt")
#     prompt = ChatPromptTemplate.from_messages(
#         [
#             ("system", template),
#             MessagesPlaceholder(variable_name="chat_history"),
#             ("human", "{input}"),
#         ]
#     )
#     # ì²´ì¸ ë§Œë“¤ê¸°
#     runnable = RunnablePassthrough.assign(
#         chat_history = RunnableLambda(memory.load_memory_variables)
#         | itemgetter("chat_history")
#     )
#     model = ChatGoogleGenerativeAI(model="gemini-1.5-pro")
#     output_parser = StrOutputParser()
#     runnable = RunnablePassthrough.assign(
#             chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")
#         )
#     chain = runnable | prompt | model | StrOutputParser()
#     # ì„¸ì…˜ ì •ë³´ ì €ì¥
#     st.session_state[key_chain] = chain
#     st.session_state[key_memory] = memory
# #-------------------------------------------------------------------
# # Chat Messages
# #-------------------------------------------------------------------
# # ì²« ì±„íŒ…ì„ ì‹œì‘í•  ë•Œ ì²« ì¸ì‚¬ ì¶œë ¥
# if len(st.session_state[key_history]) == 0:
#     greeting = "ì•ˆë…•í•˜ì„¸ìš”ğŸ˜‹"
#     st.chat_message("assistant").markdown(greeting)
#     st.session_state[key_history].append(
#         {"role":"assistant", "content": greeting}
#     )
# # ì±„íŒ… ê¸°ë¡ì´ ìˆì„ ë•Œ ê¸°ë¡ëœ ì±„íŒ… ì¶œë ¥
# else:
#     for chat in st.session_state[key_history]:
#         st.chat_message(chat["role"]).markdown(chat["content"])

# # ì±„íŒ…ì°½ ì…ë ¥
# question = st.chat_input(placeholder="ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

# if question:
#     # ì…ë ¥ëœ ì±„íŒ… ì¶œë ¥
#     st.chat_message("user").markdown(question)
#     st.session_state[key_history].append(
#         {"role":"user", "content":question}
#     )
#     # ë‹µë³€ ì¶œë ¥
#     with st.chat_message("assistant"):
#         container = st.empty()
#         answer = ""
#         inputs = {
#             "input": question,
#             "persona": persona
#         }
#         print(chain)
#         retry = 0
#         # API ì „ì†¡ ì˜¤ë¥˜ ì‹œ ìë™ ì¬ì‹œë„
#         while retry < 5:
#             try:
#                 for token in chain.stream(inputs):
#                     answer += token
#                     container.markdown(answer)
#                 break
#             except exceptions.ServiceUnavailable as e:
#                 retry += 1
#                 continue
    
#     st.session_state[key_history].append(
#         {"role":"assistant", "content":answer}
#     )
#     # ë©”ëª¨ë¦¬ ì €ì¥
#     memory.save_context(
#         {"inputs": question},
#         {"output": answer}
#     )

#     # ë©”ëª¨ë¦¬ ì¶œë ¥
#     # print(memory.load_memory_variables({}))

