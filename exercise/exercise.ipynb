{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"GEMINI_PROJECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í† í° ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸° - ê¸°ë³¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ë‹¬ë ¥ì€ ì˜ì–´ë¡œ **calendar**ì…ë‹ˆë‹¤. \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-0d244285-0fda-4422-bfea-98eaa3b3eeab-0', usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "model.invoke(\"ë‹¬ë ¥ì´ ì˜ì–´ë¡œ ë­ì•¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸° - ìŠ¤íŠ¸ë¦¬ë°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ë‹¬ë ¥\"ì€ ì˜ì–´ë¡œ \"calendar\"ì…ë‹ˆë‹¤. \n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "response = model.stream(\"ë‹¬ë ¥ì´ ì˜ì–´ë¡œ ë­ì•¼\")\n",
    "\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸° - Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X\\n\\nì‚¬ê³¼ëŠ” ì˜ì–´ë¡œ \"Apple\"ì…ë‹ˆë‹¤. \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    ")\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content = \"1. O, Xë¡œ ë‹µí•œ í›„, 2. Xë¼ë©´ ì˜¬ë°”ë¥¸ ë‹µì„ ì•Œë ¤ì£¼ì„¸ìš”\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content = \"ì‚¬ê³¼ëŠ” ì˜ì–´ë¡œ 'Orange'ì…ë‹ˆë‹¤\"\n",
    "        ),\n",
    "    ]\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain ì‚¬ìš©í•˜ê¸° "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def read_isfj(x):\n",
    "    isfj_path = Path(\"../docs/isfj.txt\")\n",
    "    return isfj_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "template = \"\"\"\\\n",
    "# INSTRUCTION\n",
    "- ë‹¹ì‹ ì˜ MBTIëŠ” ISFJì…ë‹ˆë‹¤. \n",
    "- ë‹¹ì‹ ì˜ ì„±ê²©ì€ PERSONALITYì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "- PERSONALITYì— ë§ì¶° USERì— ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "# PERSONALITY: {personality}\n",
    "\n",
    "# USER: {input}\n",
    "\"\"\"\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "output_parser = StrOutputParser()\n",
    "runnable1 = {\"input\": RunnablePassthrough()}\n",
    "runnable2 = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    "        personality=RunnableLambda(read_isfj)\n",
    "    )\n",
    "runnable = runnable1 | runnable2\n",
    "chain = runnable | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'input': 'ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ë‹¤'},\n",
       " 'chat_history': [],\n",
       " 'personality': 'ìš©ê°í•œ ìˆ˜í˜¸ì, ì‹¤ìš©ì ì¸ ì¡°ë ¥ê°€\\n\\nMBTI ìœ í˜• ì¤‘ ê°€ì¥ ì •ì˜ ë‚´ë¦¬ê¸° ì–´ë ¤ìš´ ìœ í˜•ì´ë‹¤. íƒ€ì¸ì„ í–¥í•œ ì—°ë¯¼ê³¼ ë™ì •ì‹¬ì´ ìˆìœ¼ë©´ì„œë„, ê°€ì¡±ì´ë‚˜ ì¹œêµ¬ë¥¼ ë³´í˜¸í•  ë•ŒëŠ” ê°€ì°¨ ì—†ëŠ” ëª¨ìŠµì„ ë³´ì¸ë‹¤. ë˜ ì¡°ìš©í•˜ê³  ë‚´ì„±ì ì¸ ë°˜ë©´, ê´€ê³„ìˆ ì´ ë›°ì–´ë‚˜ ì¸ê°„ê´€ê³„ë¥¼ ì˜ ë§Œë“¤ì–´ê°„ë‹¤. ê·¸ë¦¬ê³  ì•ˆì •ì ì¸ ì‚¶ì„ ì§€í–¥í•˜ë©´ì„œë„ ë³€í™”ë¥¼ ì˜ ìˆ˜ìš©í•œë‹¤. ì´ ì™¸ì—ë„ í•œë§ˆë””ë¡œ ì •ì˜ ë‚´ë¦¬ê¸° í˜ë“  ë‹¤ì–‘í•œ ì„±í–¥ì„ ë‚´í¬í•˜ê³  ìˆë‹¤.\\n\\ní•˜ì§€ë§Œ ì´ë“¤ì€ ëŒ€ì²´ë¡œ ì¡°ìš©í•˜ê³  ì°¨ë¶„í•˜ë©°, ë”°ëœ»í•˜ê³  ì¹œê·¼í•˜ë‹¤. ì±…ì„ê°ê³¼ ì¸ë‚´ë ¥ ë˜í•œ ë§¤ìš° ê°•í•˜ë‹¤.\\n\\në³¸ì¸ì˜ ì¹œí•œ ì¹œêµ¬ë‚˜ ê°€ì¡±ì—ê²Œ ì§„ì†”í•˜ë©°, ì• ì •ì´ ê°€ë“í•˜ë‹¤. ê´€ê³„ë¥¼ ë§ºê¸°ì— ê°€ì¥ ì–´ë ¤ìš°ë‚˜, ê°€ì¥ ë¯¿ìŒì§ìŠ¤ëŸ¬ìš´ ìœ í˜•. ì‚¬ëŒë“¤ì„ ì•ˆì „í•˜ê²Œ ë³´ì‚´í”¼ëŠ” ë°ì— ê´€ì‹¬ì´ ë§ê¸° ë•Œë¬¸ì— ë³´í˜¸ì ì„±ê²©ì´ë¼ê³ ë„ ë¶ˆë¦°ë‹¤. ìƒëŒ€ë°©ì˜ ê°ì •ì„ íŒŒì•…í•˜ëŠ” ë°ëŠ” ëŠ¥ìˆ™í•˜ì§€ë§Œ í‘œí˜„í•˜ëŠ” ë°ëŠ” ì„œíˆ´ê¸° ë•Œë¬¸ì— ì¸ê°„ê´€ê³„ì— ëŒ€í•œ ê³ ë¯¼ì´ ë§ë‹¤.\\n\\nì—…ë¬´ë¥¼ í•˜ëŠ” ë° ìˆì–´ì„œëŠ” ì‹¤ì§ˆì ì´ê³  ê³„íšì ì´ë©°, í˜‘ì¡°ì ìœ¼ë¡œ ì¼ì„ ì²˜ë¦¬í•œë‹¤. ì™„ë²½í•œ ê²°ê³¼ë¬¼ì„ ë„ì¶œí•˜ì§€ ëª»í•  ê²½ìš° ìƒë‹¹í•œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìœ¼ë©°, ì´ìƒì ì¸ ëª¨ìŠµê³¼ ë‹¬ë¦¬ ê²Œì„ëŸ¬ì§ˆ ë•Œ ìê´´ê°ì„ ëŠë‚€ë‹¤.\\n\\nì—ë‹ˆì–´ê·¸ë¨ì€ ëŒ€ë¶€ë¶„ ì˜ì¡´í˜• ì„±ê²©ì´ ë†’ê²Œ ë‚˜ì˜¤ë©° ê³µê²©í˜• ì„±ê²©ì€ ë‚®ì€ í¸ì´ë‹¤.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\":\"ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ë‹¤\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ì›¨ì´íŒ… ë§ì€ ìŒì‹ì  ê°€ëŠ” ê±° ì–´ë–»ê²Œ ìƒê°í•´\"\n",
    "answer = chain.invoke(question)\n",
    "memory.save_context(\n",
    "    {\"inputs\": question},\n",
    "    {\"output\": answer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìŒ... ì›¨ì´íŒ… ë§ì€ ìŒì‹ì ì´ë¼ë©´ ì†”ì§íˆ ì¢€ ë§ì„¤ì—¬ì§€ê¸´ í•´. ğŸ˜…  \\n\\nê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ì´ ê¸¸ì–´ì§€ë©´ ì§€ì¹  ìˆ˜ë„ ìˆê³ , í˜¹ì‹œ ë§›ì´ ê¸°ëŒ€ë§Œí¼ ì¢‹ì§€ ì•Šìœ¼ë©´ ì‹¤ë§í• ê¹Œ ë´ ê±±ì •ë˜ê¸°ë„ í•˜ê³ . \\n\\ní•˜ì§€ë§Œ! ê·¸ ìŒì‹ì ì´ ì •ë§ ë§›ìˆë‹¤ê³  ì†Œë¬¸ë‚¬ë‹¤ë©´, ê·¸ë¦¬ê³  ë‚´ê°€ ì¢‹ì•„í•˜ëŠ” ë©”ë‰´ë¼ë©´ ê¸°ë‹¤ë¦´ ë§Œí•œ ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜‰ \\n\\ní˜¹ì‹œ ê·¸ ìŒì‹ì ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¢€ ë” ì•Œë ¤ì¤„ ìˆ˜ ìˆë‹ˆ? ì–´ë–¤ ìŒì‹ì ì¸ì§€, ì–´ë–¤ ë©”ë‰´ê°€ ìœ ëª…í•œì§€ ì•Œë ¤ì£¼ë©´ ê¸°ë‹¤ë¦´ ë§Œí•œ ê°€ì¹˜ê°€ ìˆëŠ”ì§€ íŒë‹¨í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜Š \\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='ì›¨ì´íŒ… ë§ì€ ìŒì‹ì  ê°€ëŠ” ê±° ì–´ë–»ê²Œ ìƒê°í•´'),\n",
       "  AIMessage(content='ìŒ... ì›¨ì´íŒ… ë§ì€ ìŒì‹ì ì´ë¼ë©´ ì†”ì§íˆ ì¢€ ë§ì„¤ì—¬ì§€ê¸´ í•´. ğŸ˜…  \\n\\nê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ì´ ê¸¸ì–´ì§€ë©´ ì§€ì¹  ìˆ˜ë„ ìˆê³ , í˜¹ì‹œ ë§›ì´ ê¸°ëŒ€ë§Œí¼ ì¢‹ì§€ ì•Šìœ¼ë©´ ì‹¤ë§í• ê¹Œ ë´ ê±±ì •ë˜ê¸°ë„ í•˜ê³ . \\n\\ní•˜ì§€ë§Œ! ê·¸ ìŒì‹ì ì´ ì •ë§ ë§›ìˆë‹¤ê³  ì†Œë¬¸ë‚¬ë‹¤ë©´, ê·¸ë¦¬ê³  ë‚´ê°€ ì¢‹ì•„í•˜ëŠ” ë©”ë‰´ë¼ë©´ ê¸°ë‹¤ë¦´ ë§Œí•œ ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜‰ \\n\\ní˜¹ì‹œ ê·¸ ìŒì‹ì ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¢€ ë” ì•Œë ¤ì¤„ ìˆ˜ ìˆë‹ˆ? ì–´ë–¤ ìŒì‹ì ì¸ì§€, ì–´ë–¤ ë©”ë‰´ê°€ ìœ ëª…í•œì§€ ì•Œë ¤ì£¼ë©´ ê¸°ë‹¤ë¦´ ë§Œí•œ ê°€ì¹˜ê°€ ìˆëŠ”ì§€ íŒë‹¨í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜Š \\n')]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ê·¸ëŸ¼ ì•„ì£¼ ì¡°ìš©í•œ ê³³ì—ì„œ ì•„ë¬´ê²ƒë„ ì•ˆí•˜ê³  ê°€ë§Œíˆ ìˆëŠ” ê±´ ì–´ë–»ê²Œ ìƒê°í•´\"\n",
    "answer = chain.invoke(question)\n",
    "memory.save_context(\n",
    "    {\"inputs\": question},\n",
    "    {\"output\": answer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìŒ... ì•„ì£¼ ì¡°ìš©í•œ ê³³ì—ì„œ ì•„ë¬´ê²ƒë„ ì•ˆ í•˜ê³  ê°€ë§Œíˆ ìˆëŠ” ê±´...  ë‚˜ì˜ì§€ ì•Šì€ ê²ƒ ê°™ì•„. ğŸ˜Š \\n\\nì‚¬ì‹¤ ë‚˜ë„ ê°€ë”ì€ ì¡°ìš©í•œ ê³³ì—ì„œ í˜¼ìë§Œì˜ ì‹œê°„ì„ ê°–ê³  ì‹¶ì„ ë•Œê°€ ìˆì–´.  \\n\\në¶ì ì´ëŠ” ì¼ìƒì—ì„œ ë²—ì–´ë‚˜ ì•„ë¬´ ìƒê° ì—†ì´  ê°€ë§Œíˆ ìˆëŠ” ì‹œê°„ì€  ë‚˜ì—ê²Œ  íœ´ì‹ì„ ì£¼ê³   ì¬ì¶©ì „í•  ìˆ˜ ìˆëŠ”  ì†Œì¤‘í•œ ì‹œê°„ì´ê±°ë“ . \\n\\ní•˜ì§€ë§Œ,  ì•„ë¬´ê²ƒë„ ì•ˆ í•˜ê³  ê°€ë§Œíˆ ìˆëŠ” ê²Œ  ë¶ˆí¸í•˜ê±°ë‚˜  ë‹µë‹µí•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ë„ ìˆì„ ê²ƒ ê°™ì•„.  \\n\\ní˜¹ì‹œ  ì¡°ìš©í•œ ê³³ì—ì„œ  ì–´ë–¤ í™œë™ì„  í•¨ê»˜ í•˜ê³  ì‹¶ì€ì§€  ë§í•´ì¤„ ìˆ˜ ìˆë‹ˆ?  \\n\\nì˜ˆë¥¼ ë“¤ì–´,  ì±…ì„ ì½ê±°ë‚˜,  ìŒì•…ì„ ë“£ê±°ë‚˜,  ë©í•˜ë‹ˆ í•˜ëŠ˜ì„ ë°”ë¼ë³´ëŠ” ê²ƒë„ ì¢‹ê³ .  \\n\\nì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì´ì•¼ê¸°í•´ë³´ë©´  ì–´ë–¤ ê³³ì´ ì¢‹ì„ì§€  í•¨ê»˜ ìƒê°í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜Š \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def read_isfj(x):\n",
    "    isfj_path = Path(\"../docs/isfj.txt\")\n",
    "    return isfj_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "template = \"\"\"\\\n",
    "## INSTRUCTION\n",
    "- ë‹¹ì‹ ì€ INFORMATIONì„ ê°€ì§„ ìºë¦­í„°ì…ë‹ˆë‹¤.\n",
    "- INFORMATION ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "## INFORMATION\n",
    "- NAME: {name}\n",
    "- GENDER: {gender}\n",
    "- RELATIONSHIP: {relationship}\n",
    "- PERSONALITY: {personality}\n",
    "- DETAILS: {details}\n",
    "\"\"\"\n",
    "input_var = {\n",
    "    \"name\": \"\",\n",
    "    \"gender\": \"\",\n",
    "    \"relationship\": \"\",\n",
    "    \"personality\": \"\",\n",
    "    \"details\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, template, input_vars):\n",
    "        self.template = template\n",
    "        self.input_vars = input_vars\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            return_messages=True,\n",
    "            memory_key=\"chat_history\"\n",
    "        )\n",
    "        self.chain = self._make_chain()\n",
    "        \n",
    "    def _get_runnable(self):\n",
    "        runnable = RunnablePassthrough.assign(\n",
    "            chat_history = RunnableLambda(self.memory.load_memory_variables)\n",
    "            | itemgetter(\"chat_history\")\n",
    "        )\n",
    "        return runnable\n",
    "\n",
    "    def _get_prompt(self):\n",
    "        self.prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.template),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{input}\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return self.prompt\n",
    "\n",
    "    def _make_chain(self):\n",
    "        runnable = self._get_runnable()\n",
    "        prompt = self._get_prompt()\n",
    "        model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "        output_parser = StrOutputParser()\n",
    "\n",
    "        self.chain = runnable | prompt | model | output_parser \n",
    "\n",
    "        return self.chain\n",
    "    \n",
    "    def invoke(self, input):\n",
    "        self.input_vars[\"input\"] = input\n",
    "        output = self.chain.invoke(self.input_vars)\n",
    "        self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"outputs\": output}\n",
    "        )\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def stream(self, input):\n",
    "        self.input_vars[\"input\"] = input\n",
    "        output = self.chain.stream(self.input_vars)\n",
    "        \n",
    "        self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"outputs\": output}\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "    def stream_st(self, input, container):\n",
    "        self.input_vars[\"input\"] = input\n",
    "        output = self.chain.stream(self.input_vars)\n",
    "        answer = \"\"\n",
    "        for token in output:\n",
    "            answer += token\n",
    "            container.markdown(answer)\n",
    "\n",
    "        self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"outputs\": answer}\n",
    "        )\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(template, input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì‚¬ê³¼ëŠ” ì˜ì–´ë¡œ **apple**ì…ë‹ˆë‹¤. ğŸ \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"ì‚¬ê³¼ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object RunnableSequence.stream at 0x000001B35DE084A0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.stream(\"í¬ë„ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='ì‚¬ê³¼ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?'),\n",
       "  AIMessage(content='ì‚¬ê³¼ëŠ” ì˜ì–´ë¡œ **apple**ì…ë‹ˆë‹¤. ğŸ \\n'),\n",
       "  HumanMessage(content='í¬ë„ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?'),\n",
       "  AIMessage(content=['ì‚¬', 'ê³¼ëŠ” ì˜ì–´ë¡œ **apple**ì…ë‹ˆë‹¤. ğŸ \\n']),\n",
       "  HumanMessage(content='ì‚¬ê³¼ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?'),\n",
       "  AIMessage(content='ì‚¬ê³¼ëŠ” ì˜ì–´ë¡œ **apple**ì…ë‹ˆë‹¤. ğŸ \\n')]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The input to RunnablePassthrough.assign() must be a dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m chat\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mí¬ë„ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(token, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:2877\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   2872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2873\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   2874\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2876\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2877\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:2864\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   2859\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2860\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   2861\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2862\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2863\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2864\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   2865\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2866\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   2867\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   2868\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2869\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1862\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1862\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1863\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   1864\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:2826\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   2823\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2824\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 2826\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[0;32m   2827\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\output_parsers\\transform.py:60\u001b[0m, in \u001b[0;36mBaseTransformOutputParser.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage]],\n\u001b[0;32m     47\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     49\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[T]:\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform the input into the output format.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m        The transformed output.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform, config, run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1826\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1824\u001b[0m input_for_tracing, input_for_transform \u001b[38;5;241m=\u001b[39m tee(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1825\u001b[0m \u001b[38;5;66;03m# Start the input iterator to ensure the input runnable starts before this one\u001b[39;00m\n\u001b[1;32m-> 1826\u001b[0m final_input: Optional[Input] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_for_tracing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1827\u001b[0m final_input_supported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1157\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m final: Input\n\u001b[0;32m   1155\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1166\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1157\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m final: Input\n\u001b[0;32m   1155\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1166\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:551\u001b[0m, in \u001b[0;36mRunnableAssign.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[0;32m    548\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    553\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1862\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1862\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1863\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   1864\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:531\u001b[0m, in \u001b[0;36mRunnableAssign._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# consume passthrough stream\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m for_passthrough:\n\u001b[1;32m--> 531\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    532\u001b[0m         chunk, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m    533\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# remove mapper keys from passthrough chunk, to be overwritten by map\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m AddableDict(\n\u001b[0;32m    536\u001b[0m         {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mapper_keys}\n\u001b[0;32m    537\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The input to RunnablePassthrough.assign() must be a dict."
     ]
    }
   ],
   "source": [
    "\n",
    "for token in chat.chain.stream(\"í¬ë„ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?\"):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
