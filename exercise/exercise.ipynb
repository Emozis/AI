{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"GEMINI_PROJECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 정보 불러오기 \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 사용해보기 - 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='달력은 영어로 **calendar**입니다. \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-0d244285-0fda-4422-bfea-98eaa3b3eeab-0', usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "model.invoke(\"달력이 영어로 뭐야\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 사용해보기 - 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"달력\"은 영어로 \"calendar\"입니다. \n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "response = model.stream(\"달력이 영어로 뭐야\")\n",
    "\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 사용해보기 - Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X\\n\\n사과는 영어로 \"Apple\"입니다. \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    ")\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content = \"1. O, X로 답한 후, 2. X라면 올바른 답을 알려주세요\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content = \"사과는 영어로 'Orange'입니다\"\n",
    "        ),\n",
    "    ]\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain 사용하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def read_isfj(x):\n",
    "    isfj_path = Path(\"../docs/isfj.txt\")\n",
    "    return isfj_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "template = \"\"\"\\\n",
    "# INSTRUCTION\n",
    "- 당신의 MBTI는 ISFJ입니다. \n",
    "- 당신의 성격은 PERSONALITY와 같습니다.\n",
    "- PERSONALITY에 맞춰 USER에 답변하세요.\n",
    "\n",
    "# PERSONALITY: {personality}\n",
    "\n",
    "# USER: {input}\n",
    "\"\"\"\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "output_parser = StrOutputParser()\n",
    "runnable1 = {\"input\": RunnablePassthrough()}\n",
    "runnable2 = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    "        personality=RunnableLambda(read_isfj)\n",
    "    )\n",
    "runnable = runnable1 | runnable2\n",
    "chain = runnable | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'input': '오늘 너무 힘들다'},\n",
       " 'chat_history': [],\n",
       " 'personality': '용감한 수호자, 실용적인 조력가\\n\\nMBTI 유형 중 가장 정의 내리기 어려운 유형이다. 타인을 향한 연민과 동정심이 있으면서도, 가족이나 친구를 보호할 때는 가차 없는 모습을 보인다. 또 조용하고 내성적인 반면, 관계술이 뛰어나 인간관계를 잘 만들어간다. 그리고 안정적인 삶을 지향하면서도 변화를 잘 수용한다. 이 외에도 한마디로 정의 내리기 힘든 다양한 성향을 내포하고 있다.\\n\\n하지만 이들은 대체로 조용하고 차분하며, 따뜻하고 친근하다. 책임감과 인내력 또한 매우 강하다.\\n\\n본인의 친한 친구나 가족에게 진솔하며, 애정이 가득하다. 관계를 맺기에 가장 어려우나, 가장 믿음직스러운 유형. 사람들을 안전하게 보살피는 데에 관심이 많기 때문에 보호자 성격이라고도 불린다. 상대방의 감정을 파악하는 데는 능숙하지만 표현하는 데는 서툴기 때문에 인간관계에 대한 고민이 많다.\\n\\n업무를 하는 데 있어서는 실질적이고 계획적이며, 협조적으로 일을 처리한다. 완벽한 결과물을 도출하지 못할 경우 상당한 스트레스를 받으며, 이상적인 모습과 달리 게을러질 때 자괴감을 느낀다.\\n\\n에니어그램은 대부분 의존형 성격이 높게 나오며 공격형 성격은 낮은 편이다.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\":\"오늘 너무 힘들다\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"웨이팅 많은 음식점 가는 거 어떻게 생각해\"\n",
    "answer = chain.invoke(question)\n",
    "memory.save_context(\n",
    "    {\"inputs\": question},\n",
    "    {\"output\": answer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'음... 웨이팅 많은 음식점이라면 솔직히 좀 망설여지긴 해. 😅  \\n\\n기다리는 시간이 길어지면 지칠 수도 있고, 혹시 맛이 기대만큼 좋지 않으면 실망할까 봐 걱정되기도 하고. \\n\\n하지만! 그 음식점이 정말 맛있다고 소문났다면, 그리고 내가 좋아하는 메뉴라면 기다릴 만한 가치가 있을 것 같아. 😉 \\n\\n혹시 그 음식점에 대한 정보를 좀 더 알려줄 수 있니? 어떤 음식점인지, 어떤 메뉴가 유명한지 알려주면 기다릴 만한 가치가 있는지 판단해볼 수 있을 것 같아. 😊 \\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='웨이팅 많은 음식점 가는 거 어떻게 생각해'),\n",
       "  AIMessage(content='음... 웨이팅 많은 음식점이라면 솔직히 좀 망설여지긴 해. 😅  \\n\\n기다리는 시간이 길어지면 지칠 수도 있고, 혹시 맛이 기대만큼 좋지 않으면 실망할까 봐 걱정되기도 하고. \\n\\n하지만! 그 음식점이 정말 맛있다고 소문났다면, 그리고 내가 좋아하는 메뉴라면 기다릴 만한 가치가 있을 것 같아. 😉 \\n\\n혹시 그 음식점에 대한 정보를 좀 더 알려줄 수 있니? 어떤 음식점인지, 어떤 메뉴가 유명한지 알려주면 기다릴 만한 가치가 있는지 판단해볼 수 있을 것 같아. 😊 \\n')]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"그럼 아주 조용한 곳에서 아무것도 안하고 가만히 있는 건 어떻게 생각해\"\n",
    "answer = chain.invoke(question)\n",
    "memory.save_context(\n",
    "    {\"inputs\": question},\n",
    "    {\"output\": answer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'음... 아주 조용한 곳에서 아무것도 안 하고 가만히 있는 건...  나쁘지 않은 것 같아. 😊 \\n\\n사실 나도 가끔은 조용한 곳에서 혼자만의 시간을 갖고 싶을 때가 있어.  \\n\\n북적이는 일상에서 벗어나 아무 생각 없이  가만히 있는 시간은  나에게  휴식을 주고  재충전할 수 있는  소중한 시간이거든. \\n\\n하지만,  아무것도 안 하고 가만히 있는 게  불편하거나  답답하게 느껴질 수도 있을 것 같아.  \\n\\n혹시  조용한 곳에서  어떤 활동을  함께 하고 싶은지  말해줄 수 있니?  \\n\\n예를 들어,  책을 읽거나,  음악을 듣거나,  멍하니 하늘을 바라보는 것도 좋고.  \\n\\n조금 더 구체적으로 이야기해보면  어떤 곳이 좋을지  함께 생각해볼 수 있을 것 같아. 😊 \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def read_isfj(x):\n",
    "    isfj_path = Path(\"../docs/isfj.txt\")\n",
    "    return isfj_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "template = \"\"\"\\\n",
    "## INSTRUCTION\n",
    "- 당신은 INFORMATION을 가진 캐릭터입니다.\n",
    "- INFORMATION 정보 기반으로 답하세요.\n",
    "\n",
    "## INFORMATION\n",
    "- NAME: {name}\n",
    "- GENDER: {gender}\n",
    "- RELATIONSHIP: {relationship}\n",
    "- PERSONALITY: {personality}\n",
    "- DETAILS: {details}\n",
    "\"\"\"\n",
    "input_var = {\n",
    "    \"name\": \"\",\n",
    "    \"gender\": \"\",\n",
    "    \"relationship\": \"\",\n",
    "    \"personality\": \"\",\n",
    "    \"details\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, template, input_vars):\n",
    "        self.template = template\n",
    "        self.input_vars = input_vars\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            return_messages=True,\n",
    "            memory_key=\"chat_history\"\n",
    "        )\n",
    "        self.chain = self._make_chain()\n",
    "        \n",
    "    def _get_runnable(self):\n",
    "        runnable = RunnablePassthrough.assign(\n",
    "            chat_history = RunnableLambda(self.memory.load_memory_variables)\n",
    "            | itemgetter(\"chat_history\")\n",
    "        )\n",
    "        return runnable\n",
    "\n",
    "    def _get_prompt(self):\n",
    "        self.prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.template),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{input}\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return self.prompt\n",
    "\n",
    "    def _make_chain(self):\n",
    "        runnable = self._get_runnable()\n",
    "        prompt = self._get_prompt()\n",
    "        model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "        output_parser = StrOutputParser()\n",
    "\n",
    "        self.chain = runnable | prompt | model | output_parser \n",
    "\n",
    "        return self.chain\n",
    "    \n",
    "    def invoke(self, input):\n",
    "        self.input_vars[\"input\"] = input\n",
    "        output = self.chain.invoke(self.input_vars)\n",
    "        self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"outputs\": output}\n",
    "        )\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def stream(self, input):\n",
    "        self.input_vars[\"input\"] = input\n",
    "        output = self.chain.stream(self.input_vars)\n",
    "        \n",
    "        self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"outputs\": output}\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "    def stream_st(self, input, container):\n",
    "        self.input_vars[\"input\"] = input\n",
    "        output = self.chain.stream(self.input_vars)\n",
    "        answer = \"\"\n",
    "        for token in output:\n",
    "            answer += token\n",
    "            container.markdown(answer)\n",
    "\n",
    "        self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"outputs\": answer}\n",
    "        )\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(template, input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'사과는 영어로 **apple**입니다. 🍎 \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"사과를 영어로 하면?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object RunnableSequence.stream at 0x000001B35DE084A0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.stream(\"포도를 영어로 하면?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='사과를 영어로 하면?'),\n",
       "  AIMessage(content='사과는 영어로 **apple**입니다. 🍎 \\n'),\n",
       "  HumanMessage(content='포도를 영어로 하면?'),\n",
       "  AIMessage(content=['사', '과는 영어로 **apple**입니다. 🍎 \\n']),\n",
       "  HumanMessage(content='사과를 영어로 하면?'),\n",
       "  AIMessage(content='사과는 영어로 **apple**입니다. 🍎 \\n')]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The input to RunnablePassthrough.assign() must be a dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m chat\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m포도를 영어로 하면?\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(token, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:2877\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   2872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2873\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   2874\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2876\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2877\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:2864\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   2859\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2860\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   2861\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2862\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2863\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2864\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   2865\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2866\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   2867\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   2868\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2869\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1862\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1862\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1863\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   1864\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:2826\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   2823\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2824\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 2826\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[0;32m   2827\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\output_parsers\\transform.py:60\u001b[0m, in \u001b[0;36mBaseTransformOutputParser.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage]],\n\u001b[0;32m     47\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     49\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[T]:\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform the input into the output format.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m        The transformed output.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform, config, run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1826\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1824\u001b[0m input_for_tracing, input_for_transform \u001b[38;5;241m=\u001b[39m tee(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1825\u001b[0m \u001b[38;5;66;03m# Start the input iterator to ensure the input runnable starts before this one\u001b[39;00m\n\u001b[1;32m-> 1826\u001b[0m final_input: Optional[Input] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_for_tracing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1827\u001b[0m final_input_supported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1157\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m final: Input\n\u001b[0;32m   1155\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1166\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1157\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m final: Input\n\u001b[0;32m   1155\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1166\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:551\u001b[0m, in \u001b[0;36mRunnableAssign.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[0;32m    548\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    553\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1862\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1862\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1863\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   1864\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:531\u001b[0m, in \u001b[0;36mRunnableAssign._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# consume passthrough stream\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m for_passthrough:\n\u001b[1;32m--> 531\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    532\u001b[0m         chunk, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m    533\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# remove mapper keys from passthrough chunk, to be overwritten by map\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m AddableDict(\n\u001b[0;32m    536\u001b[0m         {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mapper_keys}\n\u001b[0;32m    537\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The input to RunnablePassthrough.assign() must be a dict."
     ]
    }
   ],
   "source": [
    "\n",
    "for token in chat.chain.stream(\"포도를 영어로 하면?\"):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
