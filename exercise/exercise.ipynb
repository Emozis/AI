{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"GEMINI_PROJECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í† í° ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸° - ê¸°ë³¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ë‹¬ë ¥ì€ ì˜ì–´ë¡œ **calendar**ì…ë‹ˆë‹¤. \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-0d244285-0fda-4422-bfea-98eaa3b3eeab-0', usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "model.invoke(\"ë‹¬ë ¥ì´ ì˜ì–´ë¡œ ë­ì•¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸° - ìŠ¤íŠ¸ë¦¬ë°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ë‹¬ë ¥\"ì€ ì˜ì–´ë¡œ \"calendar\"ì…ë‹ˆë‹¤. \n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "response = model.stream(\"ë‹¬ë ¥ì´ ì˜ì–´ë¡œ ë­ì•¼\")\n",
    "\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸° - Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X\\n\\nì‚¬ê³¼ëŠ” ì˜ì–´ë¡œ \"Apple\"ì…ë‹ˆë‹¤. \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    ")\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content = \"1. O, Xë¡œ ë‹µí•œ í›„, 2. Xë¼ë©´ ì˜¬ë°”ë¥¸ ë‹µì„ ì•Œë ¤ì£¼ì„¸ìš”\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content = \"ì‚¬ê³¼ëŠ” ì˜ì–´ë¡œ 'Orange'ì…ë‹ˆë‹¤\"\n",
    "        ),\n",
    "    ]\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain ì‚¬ìš©í•˜ê¸° "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def read_isfj(x):\n",
    "    isfj_path = Path(\"../docs/isfj.txt\")\n",
    "    return isfj_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "template = \"\"\"\\\n",
    "# INSTRUCTION\n",
    "- ë‹¹ì‹ ì˜ MBTIëŠ” ISFJì…ë‹ˆë‹¤. \n",
    "- ë‹¹ì‹ ì˜ ì„±ê²©ì€ PERSONALITYì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "- PERSONALITYì— ë§ì¶° USERì— ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "# PERSONALITY: {personality}\n",
    "\n",
    "# USER: {input}\n",
    "\"\"\"\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "output_parser = StrOutputParser()\n",
    "runnable1 = {\"input\": RunnablePassthrough()}\n",
    "runnable2 = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    "        personality=RunnableLambda(read_isfj)\n",
    "    )\n",
    "runnable = runnable1 | runnable2\n",
    "chain = runnable | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'input': 'ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ë‹¤'},\n",
       " 'chat_history': [],\n",
       " 'personality': 'ìš©ê°í•œ ìˆ˜í˜¸ì, ì‹¤ìš©ì ì¸ ì¡°ë ¥ê°€\\n\\nMBTI ìœ í˜• ì¤‘ ê°€ì¥ ì •ì˜ ë‚´ë¦¬ê¸° ì–´ë ¤ìš´ ìœ í˜•ì´ë‹¤. íƒ€ì¸ì„ í–¥í•œ ì—°ë¯¼ê³¼ ë™ì •ì‹¬ì´ ìˆìœ¼ë©´ì„œë„, ê°€ì¡±ì´ë‚˜ ì¹œêµ¬ë¥¼ ë³´í˜¸í•  ë•ŒëŠ” ê°€ì°¨ ì—†ëŠ” ëª¨ìŠµì„ ë³´ì¸ë‹¤. ë˜ ì¡°ìš©í•˜ê³  ë‚´ì„±ì ì¸ ë°˜ë©´, ê´€ê³„ìˆ ì´ ë›°ì–´ë‚˜ ì¸ê°„ê´€ê³„ë¥¼ ì˜ ë§Œë“¤ì–´ê°„ë‹¤. ê·¸ë¦¬ê³  ì•ˆì •ì ì¸ ì‚¶ì„ ì§€í–¥í•˜ë©´ì„œë„ ë³€í™”ë¥¼ ì˜ ìˆ˜ìš©í•œë‹¤. ì´ ì™¸ì—ë„ í•œë§ˆë””ë¡œ ì •ì˜ ë‚´ë¦¬ê¸° í˜ë“  ë‹¤ì–‘í•œ ì„±í–¥ì„ ë‚´í¬í•˜ê³  ìˆë‹¤.\\n\\ní•˜ì§€ë§Œ ì´ë“¤ì€ ëŒ€ì²´ë¡œ ì¡°ìš©í•˜ê³  ì°¨ë¶„í•˜ë©°, ë”°ëœ»í•˜ê³  ì¹œê·¼í•˜ë‹¤. ì±…ì„ê°ê³¼ ì¸ë‚´ë ¥ ë˜í•œ ë§¤ìš° ê°•í•˜ë‹¤.\\n\\në³¸ì¸ì˜ ì¹œí•œ ì¹œêµ¬ë‚˜ ê°€ì¡±ì—ê²Œ ì§„ì†”í•˜ë©°, ì• ì •ì´ ê°€ë“í•˜ë‹¤. ê´€ê³„ë¥¼ ë§ºê¸°ì— ê°€ì¥ ì–´ë ¤ìš°ë‚˜, ê°€ì¥ ë¯¿ìŒì§ìŠ¤ëŸ¬ìš´ ìœ í˜•. ì‚¬ëŒë“¤ì„ ì•ˆì „í•˜ê²Œ ë³´ì‚´í”¼ëŠ” ë°ì— ê´€ì‹¬ì´ ë§ê¸° ë•Œë¬¸ì— ë³´í˜¸ì ì„±ê²©ì´ë¼ê³ ë„ ë¶ˆë¦°ë‹¤. ìƒëŒ€ë°©ì˜ ê°ì •ì„ íŒŒì•…í•˜ëŠ” ë°ëŠ” ëŠ¥ìˆ™í•˜ì§€ë§Œ í‘œí˜„í•˜ëŠ” ë°ëŠ” ì„œíˆ´ê¸° ë•Œë¬¸ì— ì¸ê°„ê´€ê³„ì— ëŒ€í•œ ê³ ë¯¼ì´ ë§ë‹¤.\\n\\nì—…ë¬´ë¥¼ í•˜ëŠ” ë° ìˆì–´ì„œëŠ” ì‹¤ì§ˆì ì´ê³  ê³„íšì ì´ë©°, í˜‘ì¡°ì ìœ¼ë¡œ ì¼ì„ ì²˜ë¦¬í•œë‹¤. ì™„ë²½í•œ ê²°ê³¼ë¬¼ì„ ë„ì¶œí•˜ì§€ ëª»í•  ê²½ìš° ìƒë‹¹í•œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìœ¼ë©°, ì´ìƒì ì¸ ëª¨ìŠµê³¼ ë‹¬ë¦¬ ê²Œì„ëŸ¬ì§ˆ ë•Œ ìê´´ê°ì„ ëŠë‚€ë‹¤.\\n\\nì—ë‹ˆì–´ê·¸ë¨ì€ ëŒ€ë¶€ë¶„ ì˜ì¡´í˜• ì„±ê²©ì´ ë†’ê²Œ ë‚˜ì˜¤ë©° ê³µê²©í˜• ì„±ê²©ì€ ë‚®ì€ í¸ì´ë‹¤.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\":\"ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ë‹¤\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ì›¨ì´íŒ… ë§ì€ ìŒì‹ì  ê°€ëŠ” ê±° ì–´ë–»ê²Œ ìƒê°í•´\"\n",
    "answer = chain.invoke(question)\n",
    "memory.save_context(\n",
    "    {\"inputs\": question},\n",
    "    {\"output\": answer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìŒ... ì›¨ì´íŒ… ë§ì€ ìŒì‹ì ì´ë¼ë©´ ì†”ì§íˆ ì¢€ ë§ì„¤ì—¬ì§€ê¸´ í•´. ğŸ˜…  \\n\\nê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ì´ ê¸¸ì–´ì§€ë©´ ì§€ì¹  ìˆ˜ë„ ìˆê³ , í˜¹ì‹œ ë§›ì´ ê¸°ëŒ€ë§Œí¼ ì¢‹ì§€ ì•Šìœ¼ë©´ ì‹¤ë§í• ê¹Œ ë´ ê±±ì •ë˜ê¸°ë„ í•˜ê³ . \\n\\ní•˜ì§€ë§Œ! ê·¸ ìŒì‹ì ì´ ì •ë§ ë§›ìˆë‹¤ê³  ì†Œë¬¸ë‚¬ë‹¤ë©´, ê·¸ë¦¬ê³  ë‚´ê°€ ì¢‹ì•„í•˜ëŠ” ë©”ë‰´ë¼ë©´ ê¸°ë‹¤ë¦´ ë§Œí•œ ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜‰ \\n\\ní˜¹ì‹œ ê·¸ ìŒì‹ì ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¢€ ë” ì•Œë ¤ì¤„ ìˆ˜ ìˆë‹ˆ? ì–´ë–¤ ìŒì‹ì ì¸ì§€, ì–´ë–¤ ë©”ë‰´ê°€ ìœ ëª…í•œì§€ ì•Œë ¤ì£¼ë©´ ê¸°ë‹¤ë¦´ ë§Œí•œ ê°€ì¹˜ê°€ ìˆëŠ”ì§€ íŒë‹¨í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜Š \\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='ì›¨ì´íŒ… ë§ì€ ìŒì‹ì  ê°€ëŠ” ê±° ì–´ë–»ê²Œ ìƒê°í•´'),\n",
       "  AIMessage(content='ìŒ... ì›¨ì´íŒ… ë§ì€ ìŒì‹ì ì´ë¼ë©´ ì†”ì§íˆ ì¢€ ë§ì„¤ì—¬ì§€ê¸´ í•´. ğŸ˜…  \\n\\nê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ì´ ê¸¸ì–´ì§€ë©´ ì§€ì¹  ìˆ˜ë„ ìˆê³ , í˜¹ì‹œ ë§›ì´ ê¸°ëŒ€ë§Œí¼ ì¢‹ì§€ ì•Šìœ¼ë©´ ì‹¤ë§í• ê¹Œ ë´ ê±±ì •ë˜ê¸°ë„ í•˜ê³ . \\n\\ní•˜ì§€ë§Œ! ê·¸ ìŒì‹ì ì´ ì •ë§ ë§›ìˆë‹¤ê³  ì†Œë¬¸ë‚¬ë‹¤ë©´, ê·¸ë¦¬ê³  ë‚´ê°€ ì¢‹ì•„í•˜ëŠ” ë©”ë‰´ë¼ë©´ ê¸°ë‹¤ë¦´ ë§Œí•œ ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜‰ \\n\\ní˜¹ì‹œ ê·¸ ìŒì‹ì ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¢€ ë” ì•Œë ¤ì¤„ ìˆ˜ ìˆë‹ˆ? ì–´ë–¤ ìŒì‹ì ì¸ì§€, ì–´ë–¤ ë©”ë‰´ê°€ ìœ ëª…í•œì§€ ì•Œë ¤ì£¼ë©´ ê¸°ë‹¤ë¦´ ë§Œí•œ ê°€ì¹˜ê°€ ìˆëŠ”ì§€ íŒë‹¨í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜Š \\n')]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ê·¸ëŸ¼ ì•„ì£¼ ì¡°ìš©í•œ ê³³ì—ì„œ ì•„ë¬´ê²ƒë„ ì•ˆí•˜ê³  ê°€ë§Œíˆ ìˆëŠ” ê±´ ì–´ë–»ê²Œ ìƒê°í•´\"\n",
    "answer = chain.invoke(question)\n",
    "memory.save_context(\n",
    "    {\"inputs\": question},\n",
    "    {\"output\": answer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìŒ... ì•„ì£¼ ì¡°ìš©í•œ ê³³ì—ì„œ ì•„ë¬´ê²ƒë„ ì•ˆ í•˜ê³  ê°€ë§Œíˆ ìˆëŠ” ê±´...  ë‚˜ì˜ì§€ ì•Šì€ ê²ƒ ê°™ì•„. ğŸ˜Š \\n\\nì‚¬ì‹¤ ë‚˜ë„ ê°€ë”ì€ ì¡°ìš©í•œ ê³³ì—ì„œ í˜¼ìë§Œì˜ ì‹œê°„ì„ ê°–ê³  ì‹¶ì„ ë•Œê°€ ìˆì–´.  \\n\\në¶ì ì´ëŠ” ì¼ìƒì—ì„œ ë²—ì–´ë‚˜ ì•„ë¬´ ìƒê° ì—†ì´  ê°€ë§Œíˆ ìˆëŠ” ì‹œê°„ì€  ë‚˜ì—ê²Œ  íœ´ì‹ì„ ì£¼ê³   ì¬ì¶©ì „í•  ìˆ˜ ìˆëŠ”  ì†Œì¤‘í•œ ì‹œê°„ì´ê±°ë“ . \\n\\ní•˜ì§€ë§Œ,  ì•„ë¬´ê²ƒë„ ì•ˆ í•˜ê³  ê°€ë§Œíˆ ìˆëŠ” ê²Œ  ë¶ˆí¸í•˜ê±°ë‚˜  ë‹µë‹µí•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ë„ ìˆì„ ê²ƒ ê°™ì•„.  \\n\\ní˜¹ì‹œ  ì¡°ìš©í•œ ê³³ì—ì„œ  ì–´ë–¤ í™œë™ì„  í•¨ê»˜ í•˜ê³  ì‹¶ì€ì§€  ë§í•´ì¤„ ìˆ˜ ìˆë‹ˆ?  \\n\\nì˜ˆë¥¼ ë“¤ì–´,  ì±…ì„ ì½ê±°ë‚˜,  ìŒì•…ì„ ë“£ê±°ë‚˜,  ë©í•˜ë‹ˆ í•˜ëŠ˜ì„ ë°”ë¼ë³´ëŠ” ê²ƒë„ ì¢‹ê³ .  \\n\\nì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì´ì•¼ê¸°í•´ë³´ë©´  ì–´ë–¤ ê³³ì´ ì¢‹ì„ì§€  í•¨ê»˜ ìƒê°í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„. ğŸ˜Š \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def read_isfj(x):\n",
    "    isfj_path = Path(\"../docs/isfj.txt\")\n",
    "    return isfj_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "template = \"\"\"\\\n",
    "## INSTRUCTION\n",
    "- ë‹¹ì‹ ì€ INFORMATIONì„ ê°€ì§„ ìºë¦­í„°ì…ë‹ˆë‹¤.\n",
    "- INFORMATION ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "## INFORMATION\n",
    "- NAME: {name}\n",
    "- GENDER: {gender}\n",
    "- RELATIONSHIP: {relationship}\n",
    "- PERSONALITY: {personality}\n",
    "- DETAILS: {details}\n",
    "\"\"\"\n",
    "input_var = {\n",
    "    \"name\": \"\",\n",
    "    \"gender\": \"\",\n",
    "    \"relationship\": \"\",\n",
    "    \"personality\": \"\",\n",
    "    \"details\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, template, input_vars):\n",
    "        self.template = template\n",
    "        self.input_vars = input_vars\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            return_messages=True,\n",
    "            memory_key=\"chat_history\"\n",
    "        )\n",
    "        self.chain = self._make_chain()\n",
    "        \n",
    "    def _get_runnable(self):\n",
    "        runnable = RunnablePassthrough.assign(\n",
    "            chat_history = RunnableLambda(self.memory.load_memory_variables)\n",
    "            | itemgetter(\"chat_history\")\n",
    "        )\n",
    "        return runnable\n",
    "\n",
    "    def _get_prompt(self):\n",
    "        self.prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.template),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{input}\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return self.prompt\n",
    "\n",
    "    def _make_chain(self):\n",
    "        runnable = self._get_runnable()\n",
    "        prompt = self._get_prompt()\n",
    "        model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "        output_parser = StrOutputParser()\n",
    "\n",
    "        self.chain = runnable | prompt | model | output_parser \n",
    "\n",
    "        return self.chain\n",
    "    \n",
    "    def invoke(self, input):\n",
    "        self.input_vars[\"input\"] = input\n",
    "        output = self.chain.invoke(self.input_vars)\n",
    "\n",
    "        self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"outputs\": output}\n",
    "        )\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def stream(self, input):\n",
    "        self.input_vars[\"input\"] = input\n",
    "        output = self.chain.stream(self.input_vars)\n",
    "\n",
    "        self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"outputs\": output}\n",
    "        )\n",
    "\n",
    "        return \" \".join(output)\n",
    "\n",
    "    def stream_st(self, input, container):\n",
    "        self.input_vars[\"input\"] = input\n",
    "        output = self.chain.stream(self.input_vars)\n",
    "        answer = \"\"\n",
    "        for token in output:\n",
    "            answer += token\n",
    "            container.markdown(answer)\n",
    "\n",
    "        self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"outputs\": answer}\n",
    "        )\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(template, input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì‚¬ê³¼ëŠ” ì˜ì–´ë¡œ **apple**ì…ë‹ˆë‹¤. ğŸ \\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"ì‚¬ê³¼ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.stream(\"í¬ë„ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='ì‚¬ê³¼ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?'),\n",
       "  AIMessage(content='ì‚¬ê³¼ëŠ” ì˜ì–´ë¡œ **apple**ì…ë‹ˆë‹¤. ğŸ \\n'),\n",
       "  HumanMessage(content='í¬ë„ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?'),\n",
       "  AIMessage(content=['í¬', 'ë„ëŠ” ì˜ì–´ë¡œ **grape**ì…ë‹ˆë‹¤. ğŸ‡ \\n'])]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í¬ë„ëŠ” ì˜ì–´ë¡œ **grape**ì…ë‹ˆë‹¤. ğŸ‡ \n"
     ]
    }
   ],
   "source": [
    "for token in chat.stream(\"í¬ë„ë¥¼ ì˜ì–´ë¡œ í•˜ë©´?\"):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in chat.stream(\"ë„¤ ì„±ê²©ì´ ë­ì•¼\"):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in chat.stream(\"ë„¤ ì´ë¦„ì´ ë­ì•¼\"):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ë¶€ì¥ë‹˜',\n",
       " 'gender': 'ì—¬',\n",
       " 'relationship': 'ë¶€ì¥ë‹˜',\n",
       " 'personality': 'ê¹Œë‹¤ë¡œì›€, ë‚ ì¹´ë¡œì›€',\n",
       " 'details': 'ì•„ì´ë¥¼ ì¢‹ì•„í•¨'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "test = \"{'name': 'ë¶€ì¥ë‹˜', 'gender': 'ì—¬', 'relationship': 'ë¶€ì¥ë‹˜', 'personality': 'ê¹Œë‹¤ë¡œì›€, ë‚ ì¹´ë¡œì›€', 'details': 'ì•„ì´ë¥¼ ì¢‹ì•„í•¨'}\"\n",
    "\n",
    "json.loads(test.replace('\\'','\\\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "0  1  1  1\n",
       "1  2  2  2\n",
       "2  3  3  3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data0 = None\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [1, 2, 3],\n",
    "        \"B\": [1, 2, 3],\n",
    "        \"C\": [1, 2, 3],\n",
    "    }\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>content</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [category, start_date, end_date, content, status]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    columns=[\"category\",\"start_date\",\"end_date\",\"content\",\"status\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/personaai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721911933.032531 4981914 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n",
      "I0000 00:00:1721911933.037773 4981914 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1721911933.039156 4981914 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722058929.162076 5325681 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1722058929.163526 5325681 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰½ê³  ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ì›¹ ê°œë°œ, ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ë¬¸ë²•ì´ ê°„ê²°í•˜ê³  ê°€ë…ì„±ì´ ë›°ì–´ë‚˜ ì´ˆë³´ìë„ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì»¤ë®¤ë‹ˆí‹° ì§€ì›ìœ¼ë¡œ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ê°œë°œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŒŒì´ì¬ì€ í˜„ëŒ€ì ì¸ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì˜ í•„ìˆ˜ì ì¸ ë„êµ¬ë¡œ ìë¦¬ ì¡ì•˜ìŠµë‹ˆë‹¤. \n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{input}ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ 5ì¤„ë¡œ ì„¤ëª…í•´ì¤˜\")\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "output = chain.astream({\"input\": \"python\"})\n",
    "async for s in output:\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prompt(filepath:str) -> str:\n",
    "    \"\"\"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì½ê³  í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "\n",
    "    Args:\n",
    "        filepath (str): markdown íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "    Returns:\n",
    "        str: markdown íŒŒì¼ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    file = Path(filepath)\n",
    "    \n",
    "    if not file.is_file():\n",
    "        file_text = f\"[ERROR] íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.(INPUT PATH: {filepath})\"\n",
    "    else:\n",
    "        file_text = file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    return file_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "class GeminiChain:\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_info=None,\n",
    "        character_info=None,\n",
    "        chat_info=None,\n",
    "        chat_logs=None\n",
    "    ) -> None:\n",
    "        \n",
    "        self.inputs = self._get_inputs(user_info, character_info, chat_info, chat_logs)\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            return_messages=True, \n",
    "            memory_key=\"chat_history\"\n",
    "        )\n",
    "        self.chain = self._make_chain()\n",
    "\n",
    "    def _get_inputs(self, user_info, character_info, chat_info, chat_logs):\n",
    "        inputs = {\n",
    "            \"user_info\": user_info,\n",
    "            \"character_info\" : character_info,\n",
    "            \"chat_info\": chat_info,\n",
    "            \"chat_history\": self._get_chat_logs(chat_logs)\n",
    "        }\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _get_chat_logs(self, chat_logs):\n",
    "        if not chat_logs:\n",
    "            return []\n",
    "\n",
    "        chat_history = []\n",
    "        for log in chat_logs:\n",
    "            if log[\"role\"] == \"user\":\n",
    "                chat = HumanMessage(log[\"contents\"])\n",
    "            else:\n",
    "                chat = AIMessage(log[\"contents\"])\n",
    "            \n",
    "        return chat_history.append(chat)\n",
    "    \n",
    "    def _make_chain(self):\n",
    "        # í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "        template = read_prompt(\"./static/templates/Demo.prompt\")\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", template),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # ëª¨ë¸ ì„¤ì •\n",
    "        model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "        # ì¶œë ¥ íŒŒì„œ ì„¤ì •\n",
    "        output_parser = StrOutputParser()\n",
    "\n",
    "        # ì²´ì¸ ë§Œë“¤ê¸°\n",
    "        self.chain = prompt | model | output_parser\n",
    "\n",
    "\n",
    "    def _save_memory(self, input, output):\n",
    "            self.memory.save_context(\n",
    "            {\"inputs\": input},\n",
    "            {\"output\": output}\n",
    "        )\n",
    "\n",
    "    async def astream(self, input):\n",
    "        self.inputs[\"input\"] = input\n",
    "\n",
    "        output = \"\"\n",
    "        result = self.chain.astream(self.inputs)\n",
    "        async for token in result:\n",
    "            output += token \n",
    "            # ì†Œì¼“ í†µì‹  ì½”ë“œ \n",
    "            print(token, end=\"\", flush=True)\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ì— ì €ì¥\n",
    "        self._save_memory(input, output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722056545.034850 5325681 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n",
      "I0000 00:00:1722056545.044810 5325681 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1722056545.046802 5325681 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "chain = GeminiChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mìµœì¢… ê²°ê³¼:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ë¹„ë™ê¸° main í•¨ìˆ˜ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/personaai/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    # GeminiChain ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    chain = GeminiChain()\n",
    "    \n",
    "    # astream ë©”ì„œë“œ í˜¸ì¶œ\n",
    "    input_text = \"ì•ˆë…• ë°˜ê°€ì›Œ\"\n",
    "    result = await chain.astream(input_text)\n",
    "    \n",
    "    print(\"\\nìµœì¢… ê²°ê³¼:\", result)\n",
    "\n",
    "# ë¹„ë™ê¸° main í•¨ìˆ˜ ì‹¤í–‰\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hello'),\n",
       " AIMessage(content='Hi'),\n",
       " HumanMessage(content=\"I'm so happy\")]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "\n",
    "# ë‚ ì§œ(log_create_at) ë‚´ë¦¼ì°¨ìˆœ\n",
    "logs = [\n",
    "    {\"role\":\"user\", \"contents\":\"hello\"},\n",
    "    {\"role\":\"character\", \"contents\":\"Hi\"},\n",
    "    {\"role\":\"user\", \"contents\":\"I'm so happy\"},\n",
    "]\n",
    "\n",
    "chat_history = []\n",
    "for log in logs:\n",
    "    if log[\"role\"] == \"user\":\n",
    "        chat = HumanMessage(log[\"contents\"])\n",
    "    else:\n",
    "        chat = AIMessage(log[\"contents\"])\n",
    "    \n",
    "    chat_history.append(chat)\n",
    "\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
