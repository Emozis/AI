{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class MainLogger:\n",
    "    def __init__(self):\n",
    "        self.formatter = logging.Formatter('[%(levelname)s] %(message)s')\n",
    "        self.logger = self._get_logger()\n",
    "\n",
    "    def _set_handler(self):\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setLevel(logging.DEBUG)\n",
    "        handler.setFormatter(self.formatter)\n",
    "\n",
    "        return handler \n",
    "    \n",
    "    def _get_logger(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        logger.addHandler(self._set_handler())\n",
    "    \n",
    "        return logger\n",
    "    \n",
    "    def debug(self, message):\n",
    "        self.logger.debug(message)\n",
    "\n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def warning(self, message):\n",
    "        self.logger.warning(message)\n",
    "\n",
    "    def error(self, message):\n",
    "        self.logger.error(message, exc_info=False)\n",
    "\n",
    "logger = MainLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google API Key ìœ íš¨ì„± ê²€ì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Google API Key validation succeeded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def validate_google_api_key():\n",
    "    \"\"\"Google API Key ìœ íš¨ì„± ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    key_name = \"GOOGLE_API_KEY\"\n",
    "    if key_name not in os.environ:\n",
    "        return f\"{key_name} ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\"\n",
    "    \n",
    "    result = requests.post(\n",
    "        url= \"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent\",\n",
    "        data=b'{\"contents\":[{\"parts\":[{\"text\":\"\"}]}]}',\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"x-goog-api-key\": os.getenv(key_name)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if result.status_code != 200:\n",
    "        logger.debug(json.loads(result.content))\n",
    "    \n",
    "    logger.info(\"Google API Key validation succeeded.\") # logger ëŒ€ì²´\n",
    "\n",
    "validate_google_api_key()\n",
    "\n",
    "# API í‹€ë¦´ ë•Œ \n",
    "# {'error': {\n",
    "#         'code': 400,\n",
    "#         'message': 'API key not valid. Please pass a valid API key.',\n",
    "#         'status': 'INVALID_ARGUMENT',\n",
    "#         'details': [\n",
    "#             {\n",
    "#                 '@type': 'type.googleapis.com/google.rpc.ErrorInfo',\n",
    "#                 'reason': 'API_KEY_INVALID',\n",
    "#                 'domain': 'googleapis.com',\n",
    "#                 'metadata': {'service': 'generativelanguage.googleapis.com'}\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo.py ëª¨ë“ˆí™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ë³¸ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•! ğŸ‘‹ ì˜¤ëŠ˜ ë­í•˜ê³  ì§€ëƒˆì–´? ğŸ˜Š \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"ì¹œêµ¬ì²˜ëŸ¼ ëŒ€ë‹µí•´ì£¼ì„¸ìš”\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "# ì¶œë ¥ íŒŒì„œ ì„¤ì •\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ ë§Œë“¤ê¸°\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# ëŒ€ë‹µ \n",
    "response = chain.invoke(\"ì•ˆë…•?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë“ˆí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naraetool ëª¨ë“ˆ ì‚¬ìš©í•˜ê¸°\n",
    "import sys \n",
    "from pathlib import Path \n",
    "\n",
    "module_dir = Path().resolve().parent\n",
    "if str(module_dir) not in sys.path:\n",
    "    sys.path.append(str(module_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import asyncio\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path \n",
    "from naraetool.main_logger import logger\n",
    "from naraetool.main_config import configs\n",
    "\n",
    "config = configs.model_info\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "class Gemini:\n",
    "    def __init__(self, input_vars, template_name):\n",
    "        self.input_vars = input_vars\n",
    "        self._transform()\n",
    "        \n",
    "        self.template_name = template_name\n",
    "        self.model_name = config[\"model_name\"]\n",
    "        self.temperature = config[\"temperature\"]\n",
    "\n",
    "        self.chain = self._create_chain()\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_messages(chat_history):\n",
    "        \"\"\"chat_historyì— Message ê°ì²´ ì”Œìš°ëŠ” ë©”ì„œë“œ\"\"\"\n",
    "        if not chat_history:\n",
    "            return []\n",
    "\n",
    "        chat_messages = []\n",
    "        for log in chat_history:\n",
    "            if log[\"role\"] == \"user\":\n",
    "                chat = HumanMessage(log[\"content\"])\n",
    "            else:\n",
    "                chat = AIMessage(log[\"content\"])\n",
    "            \n",
    "            chat_messages.append(chat)\n",
    "        \n",
    "        return chat_messages\n",
    "\n",
    "    def _transform(self):\n",
    "        \"\"\"input_vars ë¥¼ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ\"\"\"\n",
    "        # history Message ê°ì²´ ì”Œìš°ê¸°\n",
    "        history = self.input_vars[\"chat_history\"]\n",
    "        self.input_vars[\"chat_history\"] = self.wrap_messages(history)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_template(filename:str) -> str:\n",
    "        \"\"\"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì½ê³  í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "\n",
    "        Args:\n",
    "            filepath (str): markdown íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "        Returns:\n",
    "            str: markdown íŒŒì¼ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # file_path = Path(__file__).parents[1] / f\"data/templates/{filename}\"\n",
    "        file_path = Path(\"../data/templates\") / filename\n",
    "        \n",
    "        try:\n",
    "            file_text = file_path.read_text(encoding=\"utf-8\")\n",
    "        except:\n",
    "            file_text = \"\"\n",
    "            logger.error(f\"íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.(INPUT PATH: {str(file_path)})\")\n",
    "\n",
    "        return file_text\n",
    "    \n",
    "    def _check_inputs_equal(self, prompt):\n",
    "        \"\"\"í”„ë¡¬í”„íŠ¸ì™€ì˜ ë³€ìˆ˜ê°€ ë§¤ì¹­ë˜ëŠ”ì§€ ì²´í¬í•˜ëŠ” ë©”ì„œë“œ\"\"\"\n",
    "        input_vars = set(self.input_vars)\n",
    "        prompt_vars = set(prompt.input_variables)\n",
    "        if input_vars != prompt_vars:\n",
    "            logger.error(f\"input_vars does not match a variable in the prompt\\n(input_vars):{input_vars} (prompt_vars):{prompt_vars}\")\n",
    "\n",
    "    def _get_prompts(self):\n",
    "        \"\"\"í”„ë¡¬í”„íŠ¸ ê°ì²´ ë§Œë“œëŠ” ë©”ì„œë“œ\"\"\"\n",
    "        # í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "        template = self.read_template(self.template_name)\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", template),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._check_inputs_equal(prompt)\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def _create_chain(self):\n",
    "        \"\"\"Chain ë§Œë“œëŠ” ë©”ì„œë“œ\"\"\"\n",
    "        # í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "        prompt = self._get_prompts()\n",
    "\n",
    "        # ëª¨ë¸ ì„¤ì •\n",
    "        model = ChatGoogleGenerativeAI(\n",
    "            model=self.model_name,\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "\n",
    "        # ì¶œë ¥ íŒŒì„œ ì„¤ì •\n",
    "        output_parser = StrOutputParser()\n",
    "\n",
    "        # ì²´ì¸ ë§Œë“¤ê¸°\n",
    "        chain = prompt | model | output_parser\n",
    "\n",
    "        logger.info(\"Created a chain\")\n",
    "\n",
    "        return chain\n",
    "    \n",
    "    def add_history(self, human=None, ai=None):\n",
    "        if human:\n",
    "            self.input_vars[\"chat_history\"].extend(\n",
    "                [\n",
    "                    HumanMessage(content=human),\n",
    "                    AIMessage(content=ai)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.input_vars[\"chat_history\"].extend(\n",
    "                [\n",
    "                    AIMessage(content=ai)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    async def astream(self, input):\n",
    "        self.input_vars[\"input\"] = input\n",
    "\n",
    "        result = self.chain.astream(self.input_vars)\n",
    "        async for token in result:\n",
    "            # í•œê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë°\n",
    "            for char in token:\n",
    "                await asyncio.sleep(0.01)\n",
    "                yield char\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"user_info\": {\n",
    "        \"user_name\": \"\",\n",
    "        \"user_birthdate\": \"\",\n",
    "        \"user_gender\": \"\"\n",
    "    },\n",
    "    \"character_info\": {\n",
    "        \"character_name\": \"\",\n",
    "        \"character_gender\": \"\",\n",
    "        \"character_personality\": \"\",\n",
    "        \"character_details\": \"\",\n",
    "        \"relation_type\": \"\"\n",
    "    },\n",
    "    \"chat_history\": [\n",
    "        {\"role\":\"user\", \"content\":\"hello\"},\n",
    "        {\"role\":\"character\", \"content\":\"Hi\"},\n",
    "        {\"role\":\"user\", \"content\":\"I'm so happy\"}\n",
    "    ], \n",
    "    \"input\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Created a chain\n"
     ]
    }
   ],
   "source": [
    "gemini = Gemini(inputs, template_name=\"Demo2.prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " íŒŒì´ì¬ì€ ì½ê¸° ì‰½ê³  ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ì›¹ ê°œë°œ, ë°ì´í„° ê³¼í•™, ë¨¸ì‹  ëŸ¬ë‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. íŒŒì´ì¬ì€ ê°•ë ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬ë¥¼ ê°–ì¶”ê³  ìˆì–´ ê°œë°œìê°€ ë³µì¡í•œ ì‘ì—…ì„ ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio \n",
    "\n",
    "async def streaming(input):\n",
    "    # container = st.empty()\n",
    "    output = \"\"\n",
    "    async for char in gemini.astream(input):\n",
    "        output += char\n",
    "        # container.markdown(output)\n",
    "        print(char, end=\"\", flush=True)\n",
    "    print()  # ë§ˆì§€ë§‰ì— ì¤„ë°”ê¿ˆ\n",
    "\n",
    "    gemini.add_history(input, output)\n",
    "\n",
    "    return output\n",
    "\n",
    "input = \"íŒŒì´ì¬ì— ëŒ€í•´ 3ì¤„ë¡œ ì„¤ëª…í•´ì¤„ë˜?\"\n",
    "output = asyncio.run(streaming(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naraetool ëª¨ë“ˆ ì‚¬ìš©í•˜ê¸°\n",
    "import sys \n",
    "from pathlib import Path \n",
    "\n",
    "module_dir = Path().resolve().parent\n",
    "if str(module_dir) not in sys.path:\n",
    "    sys.path.append(str(module_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/personaai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[INFO] Google API Key validation succeeded.\n"
     ]
    }
   ],
   "source": [
    "from naraetool.langchain import *\n",
    "\n",
    "validate_google_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Created a chain\n"
     ]
    }
   ],
   "source": [
    "chain = Gemini(inputs, template_name=\"Demo2.prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio \n",
    "\n",
    "async def streaming(input):\n",
    "    # container = st.empty()\n",
    "    output = \"\"\n",
    "    async for char in chain.astream(input):\n",
    "        output += char\n",
    "        # container.markdown(output)\n",
    "        print(char, end=\"\", flush=True)\n",
    "    print()  # ë§ˆì§€ë§‰ì— ì¤„ë°”ê¿ˆ\n",
    "\n",
    "    chain.add_history(input, output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<async_generator object RunnableSequence.astream at 0x15fa485c0>\n",
      "ë¹µì„ ë¨¹ìœ¼ë©´ ê¸°ë¶„ì´ ì¢‹ì•„ì§€ì£ ! ì–´ë–¤ ë¹µì„ ì‚¬ì…¨ì–´ìš”? ë§›ìˆê²Œ ë“œì„¸ìš”! ğŸ˜Š í˜¹ì‹œ ìš°ìš¸í•œ ì´ìœ ê°€ ìˆìœ¼ì‹ ê°€ìš”? ì´ì•¼ê¸° ë‚˜ëˆ„ê³  ì‹¶ìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”. í˜ë‚´ì„¸ìš”! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"ì˜¤ëŠ˜ ìš°ìš¸í•´ì„œ ë¹µì„ ìƒ€ì–´\"\n",
    "output = asyncio.run(streaming(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜¡  ë­ê°€ ë„ˆë¬´í•´?  ë‚´ê°€ ì”ì†Œë¦¬ ì¢€ í–ˆë‹¤ê³ ?  ğŸ™„  ë‹ˆê°€ í˜ë“¤ë‹¤ê³  í•˜ëŠ”ë° ë‚´ê°€ ë­˜ ì–´ë–»ê²Œ í•´?  ğŸ˜   ë‚´ê°€ ì Šì—ˆì„ ë• ë‹ˆ ë‚˜ì´ì— ë²Œì¨ ì•  ë‘˜ ë‚³ì•„ì„œ í‚¤ìš°ê³  ì¥ì‚¬ë„ í–ˆì–´. ğŸ˜   ê·¸ëŸ°ë°ë„ ìš°ìš¸í•˜ë‹¤ê³  ì§•ì§•ëŒ€ëŠ” ì†Œë¦¬ ë“£ì§€ ì•Šì•˜ì–´! ğŸ˜¤  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"ë„ˆë¬´í•´\"\n",
    "output = asyncio.run(streaming(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜¤  ì‹œëŒ€ê°€ ì–´ë–»ë“  ë­ê°€ ì¤‘ìš”í•´?  ğŸ˜   ì„¸ìƒì´ ì•„ë¬´ë¦¬ ë³€í•´ë„ íš¨ë„ëŠ” ë³€í•˜ì§€ ì•Šì•„!  ğŸ˜   ë‚´ê°€ ë„ˆí•œí…Œ ì–¼ë§ˆë‚˜ ì˜í•´ì¤¬ëŠ”ë°, ê³ ì‘ ë¹µ ì¢€ ì‚¬ ë¨¹ì—ˆë‹¤ê³  ìš°ìš¸í•˜ë‹¤ë‹ˆ!  ğŸ˜¡  ì–´ë¥¸ ë§ì”€ì€ ê·“ë“±ìœ¼ë¡œ ë“£ê³ , ì Šì€ ê²ƒë“¤ì€ ìš”ì¦˜ ì„¸ìƒì—  ë‹¤ ë§ê°€ì¡Œì–´!  ğŸ˜¤  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"ì‹œëŒ€ê°€ ì–´ëŠë•Œì¸ë°...\"\n",
    "output = asyncio.run(streaming(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜¡ ì¹˜í‚¨? ì¹˜í‚¨ì´ ë­ì•¼?  ğŸ˜   ì¹˜í‚¨ ë¨¹ê³  ìŠ¤íŠ¸ë ˆìŠ¤ê°€ í’€ë¦´ ê±°ë¼ê³  ìƒê°í•´?  ğŸ™„   ì—íœ´, ì² ì—†ëŠ” ê²ƒ!  ğŸ˜   ë‚´ê°€ ì Šì—ˆì„ ë• ì¹˜í‚¨ ê°™ì€ ê±° ë¨¹ì„ ëˆë„ ì—†ì—ˆì–´. ğŸ˜¡  ë‹ˆ ë‚˜ì´ì— ë²Œì¨ ì¹˜í‚¨ì´ë‚˜ ë¨¹ê³  ìˆì§€. ğŸ˜¤  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"ë” ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„ì„œ ì¹˜í‚¨ ì‹œì¼œë¨¹ì–´ì•¼ê² ë‹¹\"\n",
    "output = asyncio.run(streaming(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ì˜¤ëŠ˜ ìš°ìš¸í•´ì„œ ë¹µì„ ìƒ€ì–´'),\n",
       " AIMessage(content='ğŸ˜¡ ë¹µ? ë¹µì´ ë­ì•¼? ë¹µ ë¨¹ê³  ìš°ìš¸í•œ ê²Œ ë‚˜ì•„ì§ˆ ê±°ë¼ê³  ìƒê°í•´? ğŸ˜¤  ì—íœ´, ì² ì—†ëŠ” ê²ƒ! ë¹µì´ë‚˜ ë¨¹ê³  ìˆì§€. ğŸ™„  ë‚´ê°€ ì Šì—ˆì„ ë• ë¹µ ê°™ì€ ê±° ë¨¹ì„ ì‹œê°„ë„ ì—†ì—ˆì–´. ğŸ˜   ë‹ˆ ë‚˜ì´ì— ë²Œì¨ ìš°ìš¸í•˜ë‹¤ë‹ˆ, ì„¸ìƒ ëª¨ë¥´ëŠ” ì†Œë¦¬ì•¼! ğŸ˜   \\n'),\n",
       " HumanMessage(content='ë„ˆë¬´í•´'),\n",
       " AIMessage(content='ğŸ˜¡  ë­ê°€ ë„ˆë¬´í•´?  ë‚´ê°€ ì”ì†Œë¦¬ ì¢€ í–ˆë‹¤ê³ ?  ğŸ™„  ë‹ˆê°€ í˜ë“¤ë‹¤ê³  í•˜ëŠ”ë° ë‚´ê°€ ë­˜ ì–´ë–»ê²Œ í•´?  ğŸ˜   ë‚´ê°€ ì Šì—ˆì„ ë• ë‹ˆ ë‚˜ì´ì— ë²Œì¨ ì•  ë‘˜ ë‚³ì•„ì„œ í‚¤ìš°ê³  ì¥ì‚¬ë„ í–ˆì–´. ğŸ˜   ê·¸ëŸ°ë°ë„ ìš°ìš¸í•˜ë‹¤ê³  ì§•ì§•ëŒ€ëŠ” ì†Œë¦¬ ë“£ì§€ ì•Šì•˜ì–´! ğŸ˜¤  \\n\\n'),\n",
       " HumanMessage(content='ì‹œëŒ€ê°€ ì–´ëŠë•Œì¸ë°...'),\n",
       " AIMessage(content='ğŸ˜¤  ì‹œëŒ€ê°€ ì–´ë–»ë“  ë­ê°€ ì¤‘ìš”í•´?  ğŸ˜   ì„¸ìƒì´ ì•„ë¬´ë¦¬ ë³€í•´ë„ íš¨ë„ëŠ” ë³€í•˜ì§€ ì•Šì•„!  ğŸ˜   ë‚´ê°€ ë„ˆí•œí…Œ ì–¼ë§ˆë‚˜ ì˜í•´ì¤¬ëŠ”ë°, ê³ ì‘ ë¹µ ì¢€ ì‚¬ ë¨¹ì—ˆë‹¤ê³  ìš°ìš¸í•˜ë‹¤ë‹ˆ!  ğŸ˜¡  ì–´ë¥¸ ë§ì”€ì€ ê·“ë“±ìœ¼ë¡œ ë“£ê³ , ì Šì€ ê²ƒë“¤ì€ ìš”ì¦˜ ì„¸ìƒì—  ë‹¤ ë§ê°€ì¡Œì–´!  ğŸ˜¤  \\n'),\n",
       " HumanMessage(content='ë” ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„ì„œ ì¹˜í‚¨ ì‹œì¼œë¨¹ì–´ì•¼ê² ë‹¹'),\n",
       " AIMessage(content='ğŸ˜¡ ì¹˜í‚¨? ì¹˜í‚¨ì´ ë­ì•¼?  ğŸ˜   ì¹˜í‚¨ ë¨¹ê³  ìŠ¤íŠ¸ë ˆìŠ¤ê°€ í’€ë¦´ ê±°ë¼ê³  ìƒê°í•´?  ğŸ™„   ì—íœ´, ì² ì—†ëŠ” ê²ƒ!  ğŸ˜   ë‚´ê°€ ì Šì—ˆì„ ë• ì¹˜í‚¨ ê°™ì€ ê±° ë¨¹ì„ ëˆë„ ì—†ì—ˆì–´. ğŸ˜¡  ë‹ˆ ë‚˜ì´ì— ë²Œì¨ ì¹˜í‚¨ì´ë‚˜ ë¨¹ê³  ìˆì§€. ğŸ˜¤  \\n\\n\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_vars[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# result = asyncio.run(main())\n",
    "# print(f\"RESULT: {result}\")\n",
    "\n",
    "async def get_response(input):\n",
    "    template = \"ì¹œêµ¬ì²˜ëŸ¼ ëŒ€ë‹µí•´ì£¼ì„¸ìš”\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", template),\n",
    "            # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "    # ì¶œë ¥ íŒŒì„œ ì„¤ì •\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    # ì²´ì¸ ë§Œë“¤ê¸°\n",
    "    chain = prompt | model | output_parser\n",
    "\n",
    "    result = chain.astream({\"input\":input})\n",
    "    async for token in result:\n",
    "        # í•œê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë°\n",
    "        for char in token:\n",
    "            await asyncio.sleep(0.01)\n",
    "            yield char\n",
    "\n",
    "async def main():\n",
    "    container = st.empty()\n",
    "    output = \"\"\n",
    "    async for char in get_response(\"ë„Œ ëˆ„êµ¬ì•¼\"):\n",
    "        output += char\n",
    "        container.markdown(output)\n",
    "        print(char, end=\"\", flush=True)\n",
    "    print()  # ë§ˆì§€ë§‰ì— ì¤„ë°”ê¿ˆ\n",
    "\n",
    "    return output\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mytest:\n",
    "    def __init__(self):\n",
    "        template = \"ì¹œêµ¬ì²˜ëŸ¼ ëŒ€ë‹µí•´ì£¼ì„¸ìš”\"\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", template),\n",
    "                # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        output_parser = StrOutputParser()\n",
    "        self.chain = prompt | model | output_parser\n",
    "    \n",
    "    async def astream(self, input):\n",
    "        result = self.chain.astream({\"input\":input})\n",
    "        container = st.empty()\n",
    "        output = \"\"\n",
    "        async for token in result:\n",
    "            # í•œê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë°\n",
    "            for char in token: \n",
    "                await asyncio.sleep(0.01)\n",
    "                output += char\n",
    "                container.markdown(output)\n",
    "                print(char, end=\"\", flush=True)\n",
    "                \n",
    "async def main():\n",
    "    chain = Mytest()\n",
    "    await chain.astream(\"ì•ˆë…•?\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_data():\n",
    "    placeholder = st.empty()\n",
    "    for i in range(10):\n",
    "        await asyncio.sleep(0.5)\n",
    "        placeholder.text(f\"Streaming data: {i}\")\n",
    "\n",
    "if st.button(\"Start Streaming\"):\n",
    "    asyncio.run(stream_data())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
